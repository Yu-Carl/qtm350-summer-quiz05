{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f070b06",
   "metadata": {},
   "source": [
    "## Quiz 05 - Parallel Computing, Reproducibility, and Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6142c4",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "This quiz is based on the material covered in lectures 26 to 32. You may use\n",
    "any resources available to you, including the lecture notes and the internet.\n",
    "\n",
    "All the data required for this quiz can be found in the `data` folder within this repository. If you need to recreate the datasets, you can do so by running the Python script included in the `script-data-generation` folder.\n",
    "\n",
    "**Important:** Please start by completing Question 01 to set up the correct Python environment before proceeding with the other questions.\n",
    "\n",
    "This notebook contains the questions you need to answer.\n",
    "If possible, please submit your answers as an `.html` file on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47fa52",
   "metadata": {},
   "source": [
    "### **Question 01: Setting up the Python Environment**\n",
    "\n",
    "Before proceeding with the rest of the quiz, it is important to set up a Python environment with specific package versions to ensure compatibility and reproducibility. This quiz requires **Python 3.10** and the following packages with exact versions:\n",
    "- `dask-sql=2024.5.0`\n",
    "- `dask=2024.4.1`\n",
    "- `ipykernel=6.29.3`\n",
    "- `joblib=1.3.2`\n",
    "- `numpy=1.26.4`\n",
    "- `pandas=2.2.1`\n",
    "\n",
    "You can use tools like `conda`, `pipenv`, or `uv` to manage your environment. If you use conda, please make sure you **create the environment and install all packages in the same command**. Also, make sure to change your current environment to the new environment after creation. \n",
    "\n",
    "Write the terminal commands to accomplish this in the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43dbf581",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (231341539.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    conda create -n venv-dask -c conda-forge python=3.10 dask-sql=2024.5.0 dask=2024.4.1 ipykernel=6.29.3 joblib=1.3.2 numpy=1.26.4 pandas=2.2.1 tpot=0.12.2 prophet -y\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Please write your bash commands here. You can run them using the `!` operator or the `%%bash` magic.\n",
    "\n",
    "conda create -n venv-dask -c conda-forge python=3.10 dask-sql=2024.5.0 dask=2024.4.1 ipykernel=6.29.3 joblib=1.3.2 numpy=1.26.4 pandas=2.2.1 tpot=0.12.2 prophet -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d8e018",
   "metadata": {},
   "source": [
    "### Question 02 - Parallelising a Function with Joblib\n",
    "\n",
    "Use `joblib` to parallelise the computation of squaring numbers in a large array. Import the required packages and write code that uses four cores to parallelise the computation. Print the first 10 numbers.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "numbers = np.arange(1000000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12eea799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please write your answer here.\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "numbers = np.arange(1000000)\n",
    "\n",
    "result = Parallel(n_jobs=4)(delayed(square)(x) for x in numbers)\n",
    "result[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef94ca03",
   "metadata": {},
   "source": [
    "### Question 03 - Using Dask Arrays for Large Data\n",
    "\n",
    "Using Dask's `array` module, create a Dask array of random numbers with 10,000 rows and 10,000 columns. The array should be divided into chunks of 1,000 rows by 1,000 columns to enable efficient parallel computation. Populate the array with random numbers drawn from a normal distribution, where the mean is 0 and the standard deviation is 1. After creating the array, compute the mean, standard deviation, maximum, and minimum of the array using Dask's parallel computation capabilities. Use the `.compute()` method to execute the computations and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f76ddaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00015911072165366506\n",
      "1.000041777284891\n",
      "5.894120814154801\n",
      "-5.878393050347428\n"
     ]
    }
   ],
   "source": [
    "# Please write your answer here.\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "\n",
    "numbers = np.random.standard_normal(size = (10000,10000))\n",
    "da_a = da.from_array(numbers, chunks=(1000,1000))\n",
    "\n",
    "print(da_a.mean().compute())\n",
    "print(da_a.std().compute())\n",
    "print(da_a.max().compute())\n",
    "print(da_a.min().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47889840",
   "metadata": {},
   "source": [
    "### Question 04 - Dask DataFrame Operations with Parquet Files\n",
    "\n",
    "The `data` folder contains datasets for four countries—Brazil, India, UK, and USA—covering the years 1945 to 2023. Each country's data is stored in a separate Parquet file named after the country (`Brazil.parquet`, `India.parquet`, `UK.parquet`, `USA.parquet`). Each file contains the following columns:\n",
    "\n",
    "- `country` (string): The name of the country.\n",
    "- `year` (integer): The year of the record.\n",
    "- `gdp_per_capita` (float): The GDP per capita for that country and year.\n",
    "- `population` (integer): The population for that country and year.\n",
    "\n",
    "Using Dask's `dataframe` module, read _only the `country` and the `gdp_per_capita` columns_ from the Parquet files into a Dask DataFrame. Then, compute the mean and standard deviation of the GDP per capita for each country using Dask's parallel computation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd325641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brazil: 5496.292030611491 and 2682.494157650465\n",
      "India: 1251.704442866794 and 456.5256281158409\n",
      "UK: 27496.851363019257 and 10607.858035765952\n",
      "USA: 40189.822289795375 and 14892.455747057224\n"
     ]
    }
   ],
   "source": [
    "# Please write your answer here.\n",
    "import dask\n",
    "\n",
    "Brazil = dask.dataframe.read_parquet('data/Brazil.parquet', columns = (\"country\", \"gdp_per_capita\"))\n",
    "India = dask.dataframe.read_parquet('data/India.parquet', columns = (\"country\", \"gdp_per_capita\"))\n",
    "UK = dask.dataframe.read_parquet('data/UK.parquet', columns = (\"country\", \"gdp_per_capita\"))\n",
    "USA = dask.dataframe.read_parquet('data/USA.parquet', columns = (\"country\", \"gdp_per_capita\"))\n",
    "\n",
    "print(\"Brazil: \" + str(Brazil[\"gdp_per_capita\"].mean().compute()) + \" and \" + str(Brazil[\"gdp_per_capita\"].std().compute()))\n",
    "print(\"India: \" + str(India[\"gdp_per_capita\"].mean().compute()) + \" and \" + str(India[\"gdp_per_capita\"].std().compute()))\n",
    "print(\"UK: \" + str(UK[\"gdp_per_capita\"].mean().compute()) + \" and \" + str(UK[\"gdp_per_capita\"].std().compute()))\n",
    "print(\"USA: \" + str(USA[\"gdp_per_capita\"].mean().compute()) + \" and \" + str(USA[\"gdp_per_capita\"].std().compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be770baf",
   "metadata": {},
   "source": [
    "### Question 05 - Dask and SQL Queries\n",
    "\n",
    "Load the `data.csv` file into a Dask DataFrame and use the `dask_sql` package to perform a SQL query that selects the `country` and `gdp_per_capita` columns and filters the rows where `gdp_per_capita` is greater than 20000 in 2014. Display the results. Do not forget to register the Dask DataFrame as a SQL table with the `create_table` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d5163e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>UK</td>\n",
       "      <td>40455.486012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>USA</td>\n",
       "      <td>65386.141694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country  gdp_per_capita\n",
       "227      UK    40455.486012\n",
       "306     USA    65386.141694"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please write your answer here.\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask_sql import Context\n",
    "\n",
    "df = dd.read_csv(\"data/data.csv\")\n",
    "df = df.astype({\"country\": \"string\", \"gdp_per_capita\": \"float64\", \"year\": \"int64\"})\n",
    "c = Context()\n",
    "c.create_table(\"data\", df)\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT country, gdp_per_capita\n",
    "FROM data\n",
    "WHERE gdp_per_capita > 20000 AND year = 2014\n",
    "\"\"\"\n",
    "c.sql(query).compute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620cfde",
   "metadata": {},
   "source": [
    "### Question 06 - Parallelising a Function with Dask Delayed\n",
    "\n",
    "Suppose we need to compute the sum of squares of numbers for large ranges. The function below calculates the sum of squares from `0` up to `n-1`. Modify the given `sum_of_squares` function to use Dask's `@delayed` decorator and compute the sum of squares for each number in the numbers list in parallel. Measure and print the total execution time for the parallel computation, and print the results for each input number (as indicated in the code).\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "def sum_of_squares(n):\n",
    "    \"\"\"Compute the sum of squares from 0 to n-1.\"\"\"\n",
    "    return sum(i * i for i in range(n))\n",
    "\n",
    "numbers = [100_000_000, 200_000_000, 300_000_000, 400_000_000]\n",
    "\n",
    "# Measure the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform the computations serially\n",
    "results_serial = []\n",
    "for n in numbers:\n",
    "    result = sum_of_squares(n)\n",
    "    results_serial.append(result)\n",
    "    print(f\"Sum of squares up to {n}: {result}\")\n",
    "\n",
    "# Measure the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the total execution time\n",
    "serial_execution_time = end_time - start_time\n",
    "print(f\"Total execution time (serial): {serial_execution_time:.2f} seconds\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "24cc3239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of squares up to 100000000: Delayed('sum_of_squares-2fc3948a-95be-41d8-94be-74f9f56a2c6b')\n",
      "Sum of squares up to 200000000: Delayed('sum_of_squares-f05a89ea-48d7-49ba-8b24-fd4575be0d0c')\n",
      "Sum of squares up to 300000000: Delayed('sum_of_squares-b1d720a6-fa91-4c9d-a94c-313367c1802d')\n",
      "Sum of squares up to 400000000: Delayed('sum_of_squares-35332d5d-73a9-49c0-967a-d15c9a72ce5c')\n",
      "Total execution time (serial): 0.00 seconds\n",
      "Sum of squares up to 100000000: 333333328333333350000000\n",
      "Sum of squares up to 200000000: 2666666646666666700000000\n",
      "Sum of squares up to 300000000: 8999999955000000050000000\n",
      "Sum of squares up to 400000000: 21333333253333333400000000\n",
      "Total execution time (parallel): 60.52 seconds\n"
     ]
    }
   ],
   "source": [
    "# Please write your answer here.\n",
    "import dask\n",
    "import time\n",
    "\n",
    "@dask.delayed\n",
    "def sum_of_squares(n):\n",
    "    \"\"\"Compute the sum of squares from 0 to n-1.\"\"\"\n",
    "    return sum(i * i for i in range(n))\n",
    "\n",
    "numbers = [100_000_000, 200_000_000, 300_000_000, 400_000_000]\n",
    "\n",
    "# Measure the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform the computations serially\n",
    "results_serial = []\n",
    "for n in numbers:\n",
    "    result = sum_of_squares(n)\n",
    "    results_serial.append(result)\n",
    "    print(f\"Sum of squares up to {n}: {result}\")\n",
    "\n",
    "# Measure the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the total execution time\n",
    "serial_execution_time = end_time - start_time\n",
    "print(f\"Total execution time (serial): {serial_execution_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "start_time2 = time.time()\n",
    "results = []\n",
    "for n in numbers:\n",
    "    results.append(sum_of_squares(n))\n",
    "final_results = dask.compute(*results)\n",
    "end_time2 = time.time()\n",
    "\n",
    "for i in range(len(numbers)):\n",
    "    print(f\"Sum of squares up to {numbers[i]}: {final_results[i]}\")\n",
    "\n",
    "parallel_execution_time = end_time2 - start_time2\n",
    "print(f\"Total execution time (parallel): {parallel_execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9c5b75",
   "metadata": {},
   "source": [
    "### Question 07 - Using `pip` and `requirements.txt` for Dependency Management\n",
    "\n",
    "Explain how you can use `pip` to manage dependencies in a Python project. Describe the process of generating a `requirements.txt` file from your current environment and how to use this file to install the same packages in another environment or on a different machine. Please comment your code to explain each step. It is not necessary to run the code, but you can if you want to test it. You can use the code cell below to write your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32507297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your answer here.\n",
    "\n",
    "# You can use the command `pip freeze > requirements.txt` to generate a requirements.txt file that has a list of your installed packages with their versions, and then send the \n",
    "# requirements.txt file to another computer and install the dependencies on there using `pip install -r requirements.txt`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "### Question 08 - Writing a Simple Dockerfile\n",
    "\n",
    "Write a simple `Dockerfile` that creates a Docker image for a Python application. The application consists of a single Python script named `app.py` that prints \"Hello, World!\" when executed. The `Dockerfile` should use the official Python image as the base image, set a working directory in the container called `app`, and copy the `app.py` script into the image. When the container is run, it should execute the `app.py` script and print \"Hello, World!\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7f9697",
   "metadata": {},
   "source": [
    "#### Please write your answer here. You can use ```dockerfile to format your code.\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.11\n",
    "WORKDIR /app\n",
    "RUN echo \"print('Hello, World!')\" > app.py\n",
    "COPY app.py\n",
    "CMD [\"python\", \"app.py\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "### Question 09 - Writing a Dockerfile to Install Software on a Base Image\n",
    "\n",
    "Create a Dockerfile that starts from an Ubuntu 24.04 base image and installs the following software:\n",
    "\n",
    "- Git version 2.43.0-1ubuntu7.1\n",
    "- SQLite version 3.45.1-1ubuntu2\n",
    "\n",
    "Ensure that you specify the exact versions of the packages by checking their versions after installation. Include commands to clean up the package manager cache after installation to reduce the image size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7354df6",
   "metadata": {},
   "source": [
    "#### Please write your answer here. You can use ```dockerfile to format your code.\n",
    "\n",
    "```dockerfile\n",
    "FROM ubuntu:24.04\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends\\\n",
    "    git=1:2.43.0-1ubuntu7.1 \\\n",
    "    sqlite3=3.45.1-1ubuntu2 \\\n",
    "    apt-get clean && rm -rf /var/lib/apt/lists/*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85dab3a",
   "metadata": {},
   "source": [
    "### Question 10 - Writing a Dockerfile to Install Python and Packages on Ubuntu\n",
    "\n",
    "Write a `Dockerfile` that starts from an Ubuntu 24.04 base image, installs Python 3.12 and `pip`, and then uses `pip` to install specific versions of `numpy` (1.26.4), `pandas` (2.2.2), and `matplotlib` (3.9.2). Ensure you include commands to clean up the package manager cache after installation to reduce the image size. Set up a working directory named `app/` and configure the container to start an interactive Python shell `python3` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c422500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb9d04dc",
   "metadata": {},
   "source": [
    "#### Please write your answer here. You can use ```dockerfile to format your code.\n",
    "\n",
    "```dockerfile\n",
    "FROM ubuntu:24.04\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    python3.12=3.12.3-1ubuntu0.7 \\\n",
    "    python3-pip=24.0+dfsg-1ubuntu1.1 && \\\n",
    "    pip install numpy==1.26.4 pandas==2.2.2 matplotlib==3.9.2 && \\\n",
    "    apt-get clean && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "CMD [\"python3\"]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
